---
title: "CARPS Reproducibility Report"
output:
  html_document:
    toc: true
    toc_float: true
   
---

#### Article ID: CARPS_3-4-2015_PS
#### Pilot 1: Erik Santoro
#### Co-pilot: Tysen Dauer & Jaclyn Schwartz
#### Start date: 10/31/2017
#### End date: [Insert end date - use US format]   

-------

#### Methods summary: 
[Write a brief summary of the methods underlying the target outcomes written in your own words]

------

#### Target outcomes: 
[Insert the target outcomes identified in targetOutcomes.md]  

------

[The chunk below sets up some formatting options for the R Markdown document]

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

## Step 1: Load packages

[Some useful packages are being loaded below. You can add any additional ones you might need too.]

```{r}
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CARPSreports) # custom report functions
```

```{r Load package for repeated measures anova}

library(ez) # for repeated ANOVAs
library(effsize) #for effect size
library(compute.es) #for effect size
library(lsr) #for partial eta squared
```


## Step 2: Load data

```{r Load data}

d <- read.table("data/Bogus visual feedback alters movement_Data.tab", header=TRUE)

```

## Step 3: Tidy data

I tidied the data in two ways. First, I gathered by condition. However, when I ran the repeated measures of variance, the F stat and degrees of freedom were different (46 vs. 94). And so I tested another method -- I made each direction of rotation per participant per condition a separate trail. This gave a slightly higher F stat, but still wrong. I have commented out this second version.


```{r Tidy data}

d.tidy.1 <- d %>%
  gather(condition,rangeofmotion,starts_with("condition")) #the value various condition columns contains are the range of motion

d.tidy.1$Participant <- as.factor(d.tidy.1$Participant)
d.tidy.1$condition <- as.factor(d.tidy.1$condition)


#[For reference] Tidy 2 -- testing hypothesis of looking at participant per direction as a separate trial

# d.tidy.2 <- d %>%
#   mutate(trial = Participant * DirectionofRotation) %>% # creates a separate row at a participation * rotation
#   select(-Participant,-DirectionofRotation) %>%
#   gather(condition,rangeofmotion,starts_with("condition")) 
#   
# d.tidy.2$trial <- as.factor(d.tidy.2$trial)
# d.tidy.2$condition <- as.factor(d.tidy.2$condition)

```

## Step 4: Run analysis

### Pre-processing

1) I want to create a tidy table that groups by participant and averages across direction of rotation (e.g. left or right).

```{r Average by condition}

d.comparison <- d %>%
  group_by(Participant) %>%
  summarise(meancondition1_gain0.8 = mean(Condition1_Gain0.8),
            meancondition2_gain1 = mean(Condition2_Gain1),
            meancondition3_gain1.2 = mean(Condition3_Gain1.2)
              )
  
```


### Descriptive statistics


1) I want to find the means per condition.

```{r Means}

mean0.8 <- mean(d.comparison$meancondition1_gain0.8)
mean1 <- mean(d.comparison$meancondition2_gain1)
mean1.2 <- mean(d.comparison$meancondition3_gain1.2)

```


2) I want to find the standard deviations

```{r Standard deviations}


sd0.8 <- sd(d.comparison$meancondition1_gain0.8) 
sd1 <- sd(d.comparison$meancondition2_gain1) 
sd1.2 <- sd(d.comparison$meancondition3_gain1.2) 

sdpool0.8v1 <- sqrt((sd0.8**2 + sd1**2)/2)

sdpool1v1.2 <- sqrt((sd1.2**2 + sd1**2)/2)


```


### Inferential statistics

First, I attempt to re-create the repeated measures anova. We find that the F statistic had a major numerical error, but the p values matched (note both were p < 0.001, and noted as p = 0.001). In addition, please note that there were 94 degrees of freedom listed in the original paper, but only 46 here. Finally, I could not figure out how to calculate partial eta squared after ~30 minutes of searching.

Original Text: "The repeated measures ANOVA revealed a large overall effect of visual-proprioceptive feedback (condition) on pain-free range of motion F(2, 94) = 18.9, p < .001, p 2 = 0.29."

```{r Repeated Measures ANOVA}

modANOVA.1 <- ezANOVA(data = d.tidy.1,
                  dv = rangeofmotion,
                  wid = Participant,
                  within = .(condition),
                  within_full = DirectionofRotation, 
                  detailed = TRUE,
                  return_aov = TRUE) #returns aov object, which is useful for calculating partial eta squared

print(modANOVA.1)

#Note on within_full: this so the condition data is collapsed to mean of DirectionofRotation; becuase observations are the same, I think it's fine; https://groups.google.com/forum/#!topic/ez4r/e1MoQc06F4o


# #Version 2
# 
# modANOVA.2 <- ezANOVA(data = d.tidy.2,
#                   dv = rangeofmotion,
#                   wid = trial,
#                   within = .(condition),
#                   detailed = TRUE)
# 
# print(modANOVA.2)


#demoAnova <- ezANOVA(myData, # specify data frame
#                     dv = RT, # specify dependent variable 
#                     wid = subject, # specify the subject variable
#                     within = .(block, check), # specify within-subject variables
#                     detailed = TRUE # get a detailed table that includes SS
#                     )

#Repeated ANOVA in R: http://sherifsoliman.com/2014/12/10/ANOVA_in_R/ ; https://www.r-statistics.com/2010/04/repeated-measures-anova-with-r-tutorials/


```

```{r Repeated ANOVA Compared Values}

repanova.fstat.comp <- compareValues(reportedValue = 18.9, obtainedValue = 13.92793)

repanova.pval.comp <- compareValues(reportedValue = .001, obtainedValue = .001, isP=T)


repanova.fstat.comp
repanova.pval.comp



```

2) Second, I look at all pairwise compairsons. Since the methodology was not mentioned (e.g. either F-statistic comparisons or t tests), I used t-tests. All were the same, e.g. ; < 0.01.

Original text: "All pairwise comparisons were significant (ps < .01)."

```{r T tests}

#T-test by condition

ttest_0.8v1 <- t.test(d.comparison$meancondition1_gain0.8, d.comparison$meancondition2_gain1, paired = TRUE)

ttest_0.8v1.2 <- t.test(d.comparison$meancondition1_gain0.8, d.comparison$meancondition3_gain1.2, paired = TRUE)

ttest_1.2v1 <- t.test(d.comparison$meancondition2_gain1, d.comparison$meancondition3_gain1.2, paired = TRUE)

ttest_0.8v1
ttest_0.8v1.2
ttest_1.2v1

```

```{r Pairwise Findings}

pairwise.1 <- compareValues(reportedValue = .01, obtainedValue = .01, isP=T)

pairwise.2 <- compareValues(reportedValue = .01, obtainedValue = .01, isP=T)

pairwise.3 <- compareValues(reportedValue = .01, obtainedValue = .01, isP=T)


pairwise.1
pairwise.2
pairwise.3

```

3) Third, I compare effect sizes and related p values. The effect sizes and associated p values were all wrong.

Original Effect: 
"As shown in Figure 3, when vision understated true rotation, pain-free range of motion was increased, and this was a medium-sized effect, p = .006, d = 0.67; when vision overstated true rotation, pain-free range of motion was decreased, and this was a large effect, p = .001, d = 0.80."

```{r Effect Size and Corresponding T Tests (again)}

d0.8v1 <- (mean0.8 - mean1) / sdpool0.8v1


d1v1.2 <- (mean1.2 - mean1) / sdpool1v1.2

print(d0.8v1) #when vision understated motion, hence range of motion increased
ttest_0.8v1
print(d1v1.2) #when vision overstated motion, hence range of motion decreased
ttest_1.2v1

```

```{r Effect Size and P Value Findings}

understatement.effectsize <- compareValues(reportedValue = .67, obtainedValue = .89)

understatement.pvalue <- compareValues(reportedValue = .006, obtainedValue = .0049, isP=T)

overstatement.effectsize <- compareValues(reportedValue = .8, obtainedValue = 1.0042, isP=T)

overstatement.pvalue <- compareValues(reportedValue = .001, obtainedValue = .002, isP=T)



understatement.effectsize
understatement.pvalue
overstatement.effectsize
overstatement.pvalue


```

4) Fourth, I compare the percentage change and confidence intervals. The percentage change for the understatement was off, but the percentage change for the overstatement was right. 

Original quote: "Specifically, during visual feedback that understated true rotation, pain-free range of motion was increased by 6% (95% confidence interval, or CI = [2%, 11%]); during visual feedback that overstated true rotation, pain-free range of motion decreased by 7% (95% CI = [3%, 11%]). Therefore, our results show an overall effect of the manipulation of 13%.""

```{r Percentage Change and Confidence Intervals}

pctchng0.8v1 <- 100*((mean0.8-mean1)/mean1)
pctchng1.2v1 <- 100*((mean1.2-mean1)/mean1)
pctchngtotal <- abs(pctchng0.8v1) + abs(pctchng1.2v1)

t.test0.8v1 <- t.test(d.comparison$meancondition1_gain0.8,d.comparison$meancondition2_gain1)$conf.int
t.test1.2v1 <- t.test(d.comparison$meancondition2_gain1,d.comparison$meancondition3_gain1.2)$conf.int

#Vision understated true rotation
pctchng0.8v1
t.test0.8v1

#Vision overstated true rotation
pctchng1.2v1
t.test1.2v1

pctchngtotal

```

```{r Confidence Interval Findings}

understatement.pctchng <- compareValues(reportedValue = 6, obtainedValue = 6.54)
understatement.confidence.lower <- compareValues(reportedValue = 2, obtainedValue = 2.18)
understatement.confidence.upper <- compareValues(reportedValue = 11, obtainedValue = 10.89)

overstatement.pctchng <- compareValues(reportedValue = 7, obtainedValue = 6.875)
overstatement.confidence.lower <- compareValues(reportedValue = 3, obtainedValue = 2.79)
overstatement.confidence.upper <- compareValues(reportedValue = 11, obtainedValue = 10.96)

pctchngtotal <- compareValues(reportedValue = 13, obtainedValue = 13.42)

understatement.pctchng
understatement.confidence.lower
understatement.confidence.upper
overstatement.pctchng
overstatement.confidence.lower
overstatement.confidence.upper
pctchngtotal

```

## Step 5: Conclusion

[Include the carpsReport function below]

```{r}
# You can delete this commented text for your report, it is here to serve as a guide.
# Use the carpsReport() function in this code chunk.
# Here is a guide to the arguments you should include in the function:
# Report_Type: Enter 'pilot' or 'final'
# Article_ID: Enter the article's unique ID code
# Insufficient_Information_Errors: Enter the number of Insufficient Information Errors
# Decision_Errors Enter: the number of decision errors
# Major_Numerical_Errors: Enter the number of major numerical errors
# Time_to_Complete: Enter the estimated time to complete the report in minutes
# Author_Assistance: Enter whether author assistance was required (TRUE/FALSE)
# FOR EXAMPLE:
# carpsReport(Report_Type = "pilot", 
#             Article_ID = "ABhgyo", 
#             Insufficient_Information_Errors = 0, 
#             Decision_Errors = 1, 
#             Major_Numerical_Errors = 4, 
#             Time_to_Complete = 120, 
#             Author_Assistance = TRUE)
```

[Please also include a brief text summary describing your findings. If this reproducibility check was a failure, you should note any suggestions as to what you think the likely cause(s) might be.]

[This function will output information about the package versions used in this report:]

```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
