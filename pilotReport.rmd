---
title: "CARPS Reproducibility Report"
output:
  html_document:
    toc: true
    toc_float: true
   
---

```{r}
articleID <- "3-4-2015_PS" # insert the article ID code here e.g., "10-3-2015_PS"
reportType <- 'pilot'
pilotNames <- "Erik Santoro, Tysen Dauer, Jaclyn Schwartz" # insert the pilot's name here e.g., "Tom Hardwicke". If there are multiple pilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
copilotNames <- "" # insert the co-pilot's name here e.g., "Michael Frank". If there are multiple co-pilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
pilotTTC <- 720 # insert the pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
copilotTTC <- NA # insert the co- pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
pilotStartDate <- as.Date("10/31/2017", format = "%m/%d/%y") # insert the pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
copilotStartDate <- as.Date("", format = "%m/%d/%y") # insert the co-pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
completionDate <- as.Date("", format = "%m/%d/%y") # copilot insert the date of final report completion (after any necessary rounds of author assistance) in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
```

-------

#### Methods summary: 
This paper tested the effects of proprioceptive information -- aka our sense of body movement -- on pain; the outstanding theories on pain up to that point had covered nocioception, or internal-stimuli. To examine the effects of perception on pain, participants wore a virtual reality headset while rotating their heads until they felt pain; the distance between the center and where they felt pain as measured by degrees, the "pain-free range of motion", was the main dependent variable. Participants  rotated their head to the left and to the right for 3 conditions: perceived movement understated true movement (e.g. gain = 0.8, or virtual rotation was 80% of actual rotation), was the same as true movement (e.g. gain = 1, or 100% of actual rotation), or overstated true movement (e.g. gain = 1.2, or virutal rotation was 120% of actual rotation). The order in which participants experienced the conditions was counterbalanced. To minimize the detection of virtual reality manipulation, participants were exposed to a different visual scene for each of the 6 trials (3 conditions * 2 directions of rotation). The participants and experimenters were blinded. Finally, there were two "manipulation checks": the first piloted 9 healthy participants to find the ranges within which participants would not be able to determine virtual reality manipulation, and the second assessed the quality check of the machine.

------

#### Target outcomes: 
The repeated measures ANOVA revealed a large overall effect of visual-proprioceptive feedback (condition) on
pain-free range of motion F(2, 94) = 18.9, p < .001, η·p2 = 0.29. All pairwise comparisons were significant (ps < .01). As shown in Figure 3, when vision understated true rotation, pain-free range of motion was increased, and this
was a medium-sized effect, p = .006, d = 0.67; when vision overstated true rotation, pain-free range of motion was decreased, and this was a large effect, p = .001, d = 0.80. Specifically, during visual feedback that understated true rotation, pain-free range of motion was increased by 6% (95% confidence interval, or CI = [2%, 11%]); during visual feedback that overstated true rotation, pain-free range of motion decreased by 7% (95% CI = [3%, 11%]). Therefore, our results show an overall effect of the manipulation of 13%.

------

[The chunk below sets up some formatting options for the R Markdown document]

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

## Step 1: Load packages

[Some useful packages are being loaded below. You can add any additional ones you might need too.]

```{r}
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CARPSreports) # custom report functions

library(ez) # for repeated ANOVAs
library(effsize) #for effect size
library(compute.es) #for effect size
library(lsr) #for cohen's d
library(afex) #for effect sizes
library(schoRsch)
```

```{r}
# Prepare report object. This will be updated automatically by the reproCheck function each time values are compared.
reportObject <- data.frame(dummyRow = TRUE, reportedValue = NA, obtainedValue = NA, valueType = NA, percentageError = NA, comparisonOutcome = NA, eyeballCheck = NA)
```

## Step 2: Load data

```{r Load data}

d <- read.table("data/Bogus visual feedback alters movement_Data.tab", header=TRUE) #need header to be true

```

## Step 3: Tidy data

Here, I tidy the data and convert the variables needed in the ANOVA in a proper way.


```{r Tidy data}

#Create tidy data set

d.tidy <- d %>%
  gather(condition,rangeofmotion,starts_with("condition")) #the value various condition columns contains are the range of motion

#Need to convert following columns to proper type

d.tidy$Participant <- as.factor(d.tidy$Participant)
d.tidy$condition <- as.factor(d.tidy$condition)
d.tidy$DirectionofRotation <- as.numeric(d.tidy$DirectionofRotation)


```

## Step 4: Run analysis

### Pre-processing

1 -- For reference, I want to create a tidy table that groups by participant and averages across direction of rotation (e.g. left or right).

```{r Average by condition}

d.comparison <- d %>%
  group_by(Participant) %>%
  summarise(meancondition1_gain0.8 = mean(Condition1_Gain0.8),
            meancondition2_gain1 = mean(Condition2_Gain1),
            meancondition3_gain1.2 = mean(Condition3_Gain1.2)
              )
  
```


### Descriptive statistics


1 -- I want to find the means per condition.

```{r Means}

mean0.8 <- mean(d$Condition1_Gain0.8)
mean1 <- mean(d$Condition2_Gain1)
mean1.2 <- mean(d$Condition3_Gain1.2)



```

### Inferential statistics

1 -- I create a repeated measures ANOVa using condition as a within subjects variable, and DirectionofRotation as a between subjects variable. This achieves nearly the desired effect size and degrees of freedom, and the correct p value.

I should note that I do not quite understand why DirectionofRotation is a between subjects variable; it seems like it should be another within variabe. However, doing this just yields 46 degrees of freedom. 

Finally, after much testing (some results displayed below), I could not figure out how to extract effect size from either the ezANOVA package or the aov_ez package. Note that the ezANOVA gives the eta squared, not partial degrees of freedom, which I think is theoretically the same as there is only one independent variable? However, I am not sure on the stats!


Original Text: "The repeated measures ANOVA revealed a large overall effect of visual-proprioceptive feedback (condition) on pain-free range of motion F(2, 94) = 18.9, p < .001, ηp 2 = 0.29."

```{r Repeated Measures ANOVA}


#Repeated Measures ANOVA 
#Condition is within variable, and DirectionofRotation is the between variable; 

modANOVA <- ezANOVA(data = d.tidy,
                  dv = rangeofmotion,
                  wid = Participant,
                  within = .(condition),
                  between = DirectionofRotation, 
                  detailed = TRUE,
                  return_aov = TRUE) #returns aov object, which is supposedly useful for calculating partial eta squared, but I could not figure out

print(modANOVA)



#Attempt to create with aov_ez; Problem: between subjects factor had repeating participant id's
#aov_ez <- aov_ez("Participant","rangeofmotion",d.tidy,between = c("DirectionofRotation"),within="condition")



#Attempt to calculate partial eta squared; did not work
#anova_out(ezout = modANOVA$aov$`Participant:condition`, print = TRUE,etasq = "partial")


#Reference (46 degrees of freedom)

# modANOVA2 <- ezANOVA(data = d.tidy,
#                   dv = rangeofmotion,
#                   wid = Participant,
#                   within = .(condition),
#                   detailed = TRUE,
#                   return_aov = TRUE) #returns aov object, which is useful for calculating partial eta squared
# 
# print(modANOVA2)

#demoAnova <- ezANOVA(myData, # specify data frame
#                     dv = RT, # specify dependent variable 
#                     wid = subject, # specify the subject variable
#                     within = .(block, check), # specify within-subject variables
#                     detailed = TRUE # get a detailed table that includes SS
#                     )

#Repeated ANOVA in R: http://sherifsoliman.com/2014/12/10/ANOVA_in_R/ ; https://www.r-statistics.com/2010/04/repeated-measures-anova-with-r-tutorials/


```

```{r Repeated ANOVA Compared Values}

reportObject <- reproCheck(reportedValue = "18.9", obtainedValue = 18.63067, valueType = 'F')

reportObject <- reproCheck(reportedValue = "94", obtainedValue = 92, valueType = 'df')

reportObject <- reproCheck(reportedValue = ".001", obtainedValue = .001, valueType = 'p')

```

2 -- Second, I look at all pairwise compairsons. Since the methodology was not mentioned (e.g. either F-statistic comparisons or t tests), I used t-tests. All were the same, e.g. ; < 0.01.

Original text: "All pairwise comparisons were significant (ps < .01)."

```{r T tests}

ttest0.8v1 <- t.test(d$Condition1_Gain0.8, d$Condition2_Gain1, paired = TRUE)
ttest0.8v1.2 <- t.test(d$Condition1_Gain0.8, d$Condition3_Gain1.2, paired = TRUE)
ttest1.2v1 <- t.test(d$Condition3_Gain1.2, d$Condition2_Gain1, paired = TRUE)

ttest0.8v1
ttest0.8v1.2
ttest1.2v1

```


```{r Pairwise Findings}

reportObject <- reproCheck(reportedValue = "<.01", obtainedValue = .01, valueType = 'p', eyeballCheck = TRUE)

reportObject <- reproCheck(reportedValue = "<.01", obtainedValue = .01, valueType = 'p', eyeballCheck = TRUE)

reportObject <- reproCheck(reportedValue = "<.01", obtainedValue = .01, valueType = 'p', eyeballCheck = TRUE)

```


3 -- Third, I compare effect sizes and related p values. While the effect sizes were right the p values had major numerical errors; that said, there was no change in inference of stasticial power. In other words, there was a stastically significant difference between understating and normal as well as between overstating and normal

Original Text: "As shown in Figure 3, when vision understated true rotation, pain-free range of motion was increased, and this was a medium-sized effect, p = .006, d = 0.67; when vision overstated true rotation, pain-free range of motion was decreased, and this was a large effect, p = .001, d = 0.80."


```{r Effect size and Corresponding T Tests to Get P Values}

#Effect Size

d0.8v1 <- cohensD(d$Condition1_Gain0.8, d$Condition2_Gain1) # vision understating true rotation

d1v1.2 <- cohensD(d$Condition3_Gain1.2, d$Condition2_Gain1) # vision overstating true rotation


# FOR REFERENCE -- Consolidate by area (aka by means)
# d0.8v1.consolidated <- cohensD(d.comparison$meancondition1_gain0.8, d.comparison$meancondition2_gain1)
# d1v1.2.consolidated <- cohensD(d.comparison$meancondition3_gain1.2, d.comparison$meancondition2_gain1)


d0.8v1
ttest0.8v1
d1v1.2
ttest1.2v1



```



```{r Effect Size and P Value Findings}

reportObject <- reproCheck(reportedValue = ".67", obtainedValue = .6663313, valueType = 'd')

reportObject <- reproCheck(reportedValue = ".006", obtainedValue = .00205, valueType = 'p')

reportObject <- reproCheck(reportedValue = ".8", obtainedValue = 0.7959338, valueType = 'p')

reportObject <- reproCheck(reportedValue = ".001", obtainedValue = .0003054, valueType = 'p')

```

4 -- Fourth, I compare the percentage change and confidence intervals. The percentage change for the understatement (e.g. 0.8 vs 1) had a major numerical error, and the confidence interval for the overstatement had a minor numerical error. Overall, however, the same effects were seen.

Original quote: "Specifically, during visual feedback that understated true rotation, pain-free range of motion was increased by 6% (95% confidence interval, or CI = [2%, 11%]); during visual feedback that overstated true rotation, pain-free range of motion decreased by 7% (95% CI = [3%, 11%]). Therefore, our results show an overall effect of the manipulation of 13%.""

```{r Percentage Change and Confidence Intervals}

pctchng0.8v1 <- 100*((mean0.8-mean1)/mean1)
pctchng1.2v1 <- -100*((mean1.2-mean1)/mean1) #use negative 100 as this direction will be lower
pctchngtotal <- abs(pctchng0.8v1) + abs(pctchng1.2v1)

t.test0.8v1 <- t.test(d$Condition1_Gain0.8,d$Condition2_Gain1)$conf.int
t.test1.2v1 <- t.test(d$Condition2_Gain1,d$Condition3_Gain1.2)$conf.int

#Vision understated true rotation
pctchng0.8v1
t.test0.8v1

#Vision overstated true rotation
pctchng1.2v1
t.test1.2v1

pctchngtotal

```

```{r Confidence Interval Findings}

reportObject <- reproCheck(reportedValue = "6", obtainedValue = 6.541667, valueType = 'mean')
reportObject <- reproCheck(reportedValue = "2", obtainedValue = 2.510187, valueType = 'ci')
reportObject <- reproCheck(reportedValue = "11", obtainedValue = 10.573147, valueType = 'ci')

reportObject <- reproCheck(reportedValue = "7", obtainedValue = 6.875, valueType = 'mean')
reportObject <- reproCheck(reportedValue = "3", obtainedValue = 3.327993, valueType = 'ci')
reportObject <- reproCheck(reportedValue = "11", obtainedValue = 10.422007, valueType = 'ci')

reportObject <- reproCheck(reportedValue = "13", obtainedValue = 13.41667, valueType = 'mean')
```

## Step 5: Conclusion

In this computational reproducibility, the findings of the original paper were largely confirmed — that there was a large effect of manipulated perception on pain perception on both the understating and overstating condition. In addition, though there were minor or major numerical discrepancies across the findings, these did not affect any statistical conclusions. There are still several questions I have (below).

First of all, my repeated measures ANOVA yielded nearly the same F statistic (18.9 in paper vs 18.6 calculated) with the same p-value (<0.01); please note, however, that the degrees of freedom in my calculation were off (94 in paper and 92 calculated), and I could not figure out how to calculate the effect size. Second of all, all pairwise comparisons among the three conditions were the same (p<0.01); note that the author did not specify the type of pair-wise and I just assumed t-tests (rather than assuming insufficient information). Third of all, while the effect sizes for understating vision (virtual rotation was 80% of actual rotation) and overstating vision (virtual rotation was 80% of actual rotation), the p-values had major errors (0.006 vs 0.002; 0.001 vs 0.0003) that did not affect statistical conclusions (e.g. can still conclude the effect was significant). Fourth of all, while there were several errors in changes in range of motion — a major numerical error in the estimation of the percentage increase in range of motion for the understating range of motion condition (6% paper vs. 6.5% calculated), a minor numerical error in the lower confidence interval estimate for the range of motion increase for the understating condition (2% paper vs. 2.5% calculation), and a minor numerical error int he upper confidence interval estimate for the range of motion decrease for the overstating condition (11% paper vs. 10.4% calculation) — these did not effect the general conclusions for percentage changes and confidence intervals. Please note that I did not record the repeated measures ANOVA errors as insufficient errors, but as minor numerical errors. 

My outstanding questions are: (a) how did the authors get 94 not 92 degrees of freedom?, (b) why did they seemingly use direction of rotation as a between-subjects variable, (c) how do you extract effect size for a repeated measures anova, and (d) why did the numerical errors that happened happen?

```{r}
Author_Assistance = FALSE # was author assistance provided? (if so, enter TRUE)

Insufficient_Information_Errors <- 0 # how many discrete insufficient information issues did you encounter?

# Assess the causal locus (discrete reproducibility issues) of any reproducibility errors. Note that there doesn't necessarily have to be a one-to-one correspondance between discrete reproducibility issues and reproducibility errors. For example, it could be that the original article neglects to mention that a Greenhouse-Geisser correct was applied to ANOVA outcomes. This might result in multiple reproducibility errors, but there is a single causal locus (discrete reproducibility issue).

locus_typo <- NA # how many discrete issues did you encounter that related to typographical errors?
locus_specification <- NA # how many discrete issues did you encounter that related to incomplete, incorrect, or unclear specification of the original analyses?
locus_analysis <- NA # how many discrete issues did you encounter that related to errors in the authors' original analyses?
locus_data <- NA # how many discrete issues did you encounter that related to errors in the data files shared by the authors?
locus_unidentified <- NA # how many discrete issues were there for which you could not identify the cause

Affects_Conclusion <- NA # Do any reproducibility issues encounter appear to affect the conclusions made in the original article? This is a subjective judgement, but you should taking into account multiple factors, such as the presence/absence of decision errors, the number of target outcomes that could not be reproduced, the type of outcomes that could or could not be reproduced, the difference in magnitude of effect sizes, and the predictions of the specific hypothesis under scrutiny.
```

```{r}
reportObject <- reportObject %>%
  filter(dummyRow == FALSE) %>% # remove the dummy row
  select(-dummyRow) %>% # remove dummy row designation
  mutate(articleID = articleID) %>% # add the articleID 
  select(articleID, everything()) # make articleID first column

# decide on final outcome
if(any(reportObject$comparisonOutcome != "MATCH") | Insufficient_Information_Errors > 0){
  finalOutcome <- "Failure without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Failure despite author assistance"
  }
}else{
  finalOutcome <- "Success without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Success with author assistance"
  }
}

# collate report extra details
reportExtras <- data.frame(articleID, pilotNames, copilotNames, pilotTTC, copilotTTC, pilotStartDate, copilotStartDate, completionDate, Author_Assistance, finalOutcome, Insufficient_Information_Errors, locus_typo, locus_specification, locus_analysis, locus_data, locus_unidentified)

# save report objects
if(reportType == "pilot"){
  write_csv(reportObject, "pilotReportDetailed.csv")
  write_csv(reportExtras, "pilotReportExtras.csv")
}

if(reportType == "final"){
  write_csv(reportObject, "finalReportDetailed.csv")
  write_csv(reportExtras, "finalReportExtras.csv")
}
```

# Session information

```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
