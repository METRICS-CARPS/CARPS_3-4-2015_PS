---
title: "CARPS Reproducibility Report"
output:
  html_document:
    toc: true
    toc_float: true
   
---

```{r}
articleID <- "3-4-2015_PS" # insert the article ID code here e.g., "10-3-2015_PS"
reportType <- 'pilot'
pilotNames <- "Erik Santoro, Tysen Dauer, Jaclyn Schwartz" # insert the pilot's name here e.g., "Tom Hardwicke". If there are multiple pilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
copilotNames <- "Erica Yoon" # insert the co-pilot's name here e.g., "Michael Frank". If there are multiple co-pilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
pilotTTC <- 720 # insert the pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
copilotTTC <- 120 # insert the co- pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
pilotStartDate <- as.Date("10/31/17", format = "%m/%d/%y") # insert the pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
copilotStartDate <- as.Date("06/13/18", format = "%m/%d/%y") # insert the co-pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
completionDate <- as.Date("06/13/18", format = "%m/%d/%y") # copilot insert the date of final report completion (after any necessary rounds of author assistance) in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
```

-------

#### Methods summary: 
This paper tested the effects of proprioceptive information -- aka our sense of body movement -- on pain; the outstanding theories on pain up to that point had covered nocioception, or internal-stimuli. To examine the effects of perception on pain, participants wore a virtual reality headset while rotating their heads until they felt pain; the distance between the center and where they felt pain as measured by degrees, the "pain-free range of motion", was the main dependent variable. Participants rotated their head to the left and to the right for 3 conditions: perceived movement understated true movement (e.g. gain = 0.8, or virtual rotation was 80% of actual rotation), was the same as true movement (e.g. gain = 1, or 100% of actual rotation), or overstated true movement (e.g. gain = 1.2, or virutal rotation was 120% of actual rotation). The order in which participants experienced the conditions was counterbalanced. To minimize the detection of virtual reality manipulation, participants were exposed to a different visual scene for each of the 6 trials (3 conditions * 2 directions of rotation). The participants and experimenters were blinded. Finally, there were two "manipulation checks": the first piloted 9 healthy participants to find the ranges within which participants would not be able to determine virtual reality manipulation, and the second assessed the quality check of the machine.

------

#### Target outcomes: 
The repeated measures ANOVA revealed a large overall effect of visual-proprioceptive feedback (condition) on pain-free range of motion F(2, 94) = 18.9, p < .001, η·p2 = 0.29. All pairwise comparisons were significant (ps < .01). As shown in Figure 3, when vision understated true rotation, pain-free range of motion was increased, and this was a medium-sized effect, p = .006, d = 0.67; when vision overstated true rotation, pain-free range of motion was decreased, and this was a large effect, p = .001, d = 0.80. Specifically, during visual feedback that understated true rotation, pain-free range of motion was increased by 6% (95% confidence interval, or CI = [2%, 11%]); during visual feedback that overstated true rotation, pain-free range of motion decreased by 7% (95% CI = [3%, 11%]). Therefore, our results show an overall effect of the manipulation of 13%.

------

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

## Step 1: Load packages

```{r}
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CARPSreports) # custom report functions

library(ez) # for repeated ANOVAs
library(lsr) #for cohen's d
library(MBESS) # for effect size from ezANOVA
```

```{r}
# Prepare report object. This will be updated automatically by the reproCheck function each time values are compared.
reportObject <- data.frame(dummyRow = TRUE, reportedValue = NA, obtainedValue = NA, valueType = NA, percentageError = NA, comparisonOutcome = NA, eyeballCheck = NA)
```

## Step 2: Load data

According to the author (email correspondence), different normalization procedures were used for the ANOVA and t-tests. For ANOVA, the authors  "transformed data for each participant to a proportion of the average range of motion demonstrated in the neutral condition". For t-tests, the authors normalised each participants data to an average of the three conditions.

```{r Load data}
d <- read_sav("data/BogusVisualFeedbackData.sav")

d_anova <- d %>%
  select(Participant, DirectionofRotation, Condition1_Gain0.8, Condition2_Gain1, Condition3_Gain1.2)

d_ttest <- d %>%
  select(Participant, DirectionofRotation, Point.8, One1, One1.2)
```

## Step 3: Tidy data

Make a long-form data for ANOVA.

```{r Tidy data}
#Create tidy data set

d.tidy_anova <- d_anova %>%
  gather(condition,rangeofmotion,starts_with("condition")) %>% #the value various condition columns contains are the range of motion
  #Need to convert following columns to proper type
  mutate(Participant = as.factor(Participant),
         condition = as.factor(condition),
         DirectionofRotation = as.factor(DirectionofRotation))

d.tidy_ttest <- d_ttest %>%
  rename(Condition1_Gain0.8 = Point.8,
         Condition2_Gain1.0 = One1,
         Condition3_Gain1.2 = One1.2
         ) %>%
  mutate(Participant = as.factor(Participant),
         DirectionofRotation = as.factor(DirectionofRotation))
```

## Step 4: Run analysis

### Pre-processing

1 -- For reference, make a tidy table that groups by participant and averages across direction of rotation (e.g. left or right).

```{r Average by condition}

d.comparison <- d_anova %>%
  group_by(Participant) %>%
  summarise(meancondition1_gain0.8 = mean(Condition1_Gain0.8),
            meancondition2_gain1 = mean(Condition2_Gain1),
            meancondition3_gain1.2 = mean(Condition3_Gain1.2)
              )
  
```


### Descriptive statistics


1 -- Here are means of pain-free range of motion per condition.

```{r Means}

mean0.8 <- mean(d_anova$Condition1_Gain0.8)
mean1 <- mean(d_anova$Condition2_Gain1)
mean1.2 <- mean(d_anova$Condition3_Gain1.2)

```

### Inferential statistics

1 -- We create a repeated measures ANOVA using condition as a within subjects variable, and DirectionofRotation as a between subjects variable. This achieves nearly the desired effect size and degrees of freedom, and the correct p value.

Note that, according to the author (in an email correspondence), each participant x direction of rotation was considered to be a unique case.

> The repeated measures ANOVA revealed a large overall effect of visual-proprioceptive feedback (condition) on pain-free range of motion F(2, 94) = 18.9, p < .001, ηp 2 = 0.29. (from Harvie et al. p.388)

```{r Repeated Measures ANOVA}
#Repeated Measures ANOVA 
#Condition is within variable, and DirectionofRotation is the between variable; 
#Note: DirectionofRotation SHOULD be a within variable... 

modANOVA <- ezANOVA(data = d.tidy_anova %>%
                      mutate(participant_direction = paste(Participant, DirectionofRotation, sep = "_")),
                  dv = rangeofmotion,
                  # wid = Participant,
                  wid = participant_direction,
                  within = .(condition),
                  # within = .(condition, DirectionofRotation),
                  detailed = TRUE,
                  return_aov = TRUE) #returns aov object, which is supposedly useful for calculating partial eta squared, but I could not figure out
```

```{r partial_eta_squared}
# calculating partial eta squared 
# source: https://groups.google.com/forum/#!topic/ez4r/4CHBP-jlZGY
modANOVA$ANOVA$partialetasquared <- modANOVA$ANOVA$SSn/(modANOVA$ANOVA$SSn+modANOVA$ANOVA$SSd)
loweretasquared <- c()
upperetasquared <- c()
for (cR in 1:nrow(modANOVA$ANOVA)) {
  Lims <- conf.limits.ncf(F.value = modANOVA$ANOVA$F[cR], conf.level = 0.95, df.1 <- modANOVA$ANOVA$DFn[cR], df.2 <- modANOVA$ANOVA$DFd[cR])
  Lower.lim <- Lims$Lower.Limit/(Lims$Lower.Limit + df.1 + df.2 + 1)
  Upper.lim <- Lims$Upper.Limit/(Lims$Upper.Limit + df.1 + df.2 + 1)
  if (is.na(Lower.lim)) {
    Lower.lim <- 0
  }
  if (is.na(Upper.lim)) {
    Upper.lim <- 1
  }
  loweretasquared <- c(loweretasquared,Lower.lim)
  upperetasquared <- c(upperetasquared,Upper.lim)
}
modANOVA$ANOVA$partialetasquared.lower <- loweretasquared
modANOVA$ANOVA$partialetasquared.upper <- upperetasquared
```

```{r Repeated ANOVA Compared Values}
reportObject <- reproCheck(reportedValue = "18.9", obtainedValue = modANOVA$ANOVA$F[2], valueType = 'F')

reportObject <- reproCheck(reportedValue = "94", obtainedValue = modANOVA$ANOVA$DFd[2], valueType = 'df')

reportObject <- reproCheck(reportedValue = "<.001", obtainedValue = modANOVA$ANOVA$p[2], valueType = 'p', eyeballCheck = TRUE)
```

2 -- Second, we look at all pairwise comparisons, which were t-tests as confirmed by the authors in an email correspondence. All were < 0.01. We additionally checked Bonferroni corrected t-tests, and these all also yielded ps < .01.

> All pairwise comparisons were significant (ps < .01). (from Harvie et al. p.388)

```{r T tests}
ttest0.8v1 <- t.test(d.tidy_ttest$Condition1_Gain0.8, d.tidy_ttest$Condition2_Gain1.0, paired = TRUE)
ttest0.8v1.2 <- t.test(d.tidy_ttest$Condition1_Gain0.8, d.tidy_ttest$Condition3_Gain1.2, paired = TRUE)
ttest1.2v1 <- t.test(d.tidy_ttest$Condition3_Gain1.2, d.tidy_ttest$Condition2_Gain1.0, paired = TRUE)
```


```{r Pairwise Findings}

reportObject <- reproCheck(reportedValue = "<.01", obtainedValue = ttest0.8v1$p.value, valueType = 'p', eyeballCheck = TRUE)

reportObject <- reproCheck(reportedValue = "<.01", obtainedValue = ttest0.8v1.2$p.value, valueType = 'p', eyeballCheck = TRUE)

reportObject <- reproCheck(reportedValue = "<.01", obtainedValue = ttest1.2v1$p.value, valueType = 'p', eyeballCheck = TRUE)

```


3 -- Third, we compare effect sizes and related p values. Effect sizes were not replicated -- Cohen's d for both comparisons were lower than the authors reported. The p-values were generally replicated, though one was found to be a major error probably due to a rounding issue or misnomer (e.g., "=" intended to mean "<" in the paper).

> As shown in Figure 3, when vision understated true rotation, pain-free range of motion was increased, and this was a medium-sized effect, p = .006, d = 0.67; when vision overstated true rotation, pain-free range of motion was decreased, and this was a large effect, p = .001, d = 0.80.  (from Harvie et al. p.388-9)


```{r Effect size and Corresponding T Tests to Get P Values}
ttest_cohen1vs2 <- pairedSamplesTTest(formula = ~Condition1_Gain0.8 + Condition2_Gain1.0, data=as.data.frame(d.tidy_ttest))

ttest_cohen2vs3 <- pairedSamplesTTest(formula = ~Condition2_Gain1.0 + Condition3_Gain1.2, data=as.data.frame(d.tidy_ttest))
```



```{r Effect Size and P Value Findings}

reportObject <- reproCheck(reportedValue = ".67", obtainedValue = ttest_cohen1vs2$effect.size, valueType = 'd')

reportObject <- reproCheck(reportedValue = ".006", obtainedValue = ttest_cohen1vs2$p.value, valueType = 'p')

reportObject <- reproCheck(reportedValue = ".8", obtainedValue = ttest_cohen2vs3$effect.size, valueType = 'd')

reportObject <- reproCheck(reportedValue = ".001", obtainedValue = ttest_cohen2vs3$p.value, valueType = 'p')

```

4 -- Fourth, we compare the percentage change and confidence intervals. The confidence interval for the understatement (e.g. 0.8 vs 1) had a major numerical error, and the confidence interval for the overstatement had two major numerical errors. Overall, however, the same effects were seen.

> Specifically, during visual feedback that understated true rotation, pain-free range of motion was increased by 6% (95% confidence interval, or CI = [2%, 11%]); during visual feedback that overstated true rotation, pain-free range of motion decreased by 7% (95% CI = [3%, 11%]). Therefore, our results show an overall effect of the manipulation of 13%." (from Harvie et al. p.389)

```{r Percentage Change and Confidence Intervals}
#Vision understated true rotation
confint_1vs2 <- (ttest_cohen1vs2$conf.int)*100
mean_1vs2 <- (ttest_cohen1vs2$mean[1] - ttest_cohen1vs2$mean[2])*100

#Vision overstated true rotation
confint_2vs3 <- (ttest_cohen2vs3$conf.int)*100
mean_2vs3 <- (ttest_cohen2vs3$mean[1] - ttest_cohen2vs3$mean[2])*100
```

```{r Confidence Interval Findings}

reportObject <- reproCheck(reportedValue = "6", obtainedValue = mean_1vs2, valueType = 'mean')
reportObject <- reproCheck(reportedValue = "2", obtainedValue = confint_1vs2[1], valueType = 'ci')
reportObject <- reproCheck(reportedValue = "11", obtainedValue = confint_1vs2[2], valueType = 'ci')

reportObject <- reproCheck(reportedValue = "7", obtainedValue = mean_2vs3, valueType = 'mean')
reportObject <- reproCheck(reportedValue = "3", obtainedValue = confint_2vs3[1], valueType = 'ci')
reportObject <- reproCheck(reportedValue = "11", obtainedValue = confint_2vs3[2], valueType = 'ci')

reportObject <- reproCheck(reportedValue = "13", obtainedValue = mean_1vs2+mean_2vs3, valueType = 'mean')
```

## Step 5: Conclusion

In this computational reproducibility, the findings of the original paper were largely confirmed — that there was a large effect of manipulated perception on pain perception on both the understating and overstating condition. In addition, though there were minor or major numerical discrepancies across the findings, these did not affect any statistical conclusions. There are still several questions we have (below).

First of all, our repeated measures ANOVA yielded the same F statistic and the same p-value (<0.01). Second of all, all pairwise comparisons among the three conditions were the same as reported (p<0.01). Third of all, the effect sizes were not replicated for understating vision (virtual rotation was 80% of actual rotation) and overstating vision (virtual rotation was 80% of actual rotation) and were found to be lower than reported, while the p-values were replicated and thus did not affect statistical conclusions. Fourth of all, while there were several errors in changes in range of motion — three major errors in confidence interval computation — which did not effect the general conclusions for percentage changes and confidence intervals and plausibly come from minor rounding issues.

An outstanding question is, what is causing the discrepancy between the reported and currently computed effect sizes?

```{r}
Author_Assistance = FALSE # was author assistance provided? (if so, enter TRUE)

Insufficient_Information_Errors <- 0 # how many discrete insufficient information issues did you encounter?

# Assess the causal locus (discrete reproducibility issues) of any reproducibility errors. Note that there doesn't necessarily have to be a one-to-one correspondance between discrete reproducibility issues and reproducibility errors. For example, it could be that the original article neglects to mention that a Greenhouse-Geisser correct was applied to ANOVA outcomes. This might result in multiple reproducibility errors, but there is a single causal locus (discrete reproducibility issue).

locus_typo <- NA # how many discrete issues did you encounter that related to typographical errors?
locus_specification <- 1 # how many discrete issues did you encounter that related to incomplete, incorrect, or unclear specification of the original analyses?
locus_analysis <- 2 # how many discrete issues did you encounter that related to errors in the authors' original analyses?
locus_data <- NA # how many discrete issues did you encounter that related to errors in the data files shared by the authors?
locus_unidentified <- NA # how many discrete issues were there for which you could not identify the cause

Affects_Conclusion <- NA # Do any reproducibility issues encounter appear to affect the conclusions made in the original article? This is a subjective judgement, but you should taking into account multiple factors, such as the presence/absence of decision errors, the number of target outcomes that could not be reproduced, the type of outcomes that could or could not be reproduced, the difference in magnitude of effect sizes, and the predictions of the specific hypothesis under scrutiny.
```

```{r}
reportObject <- reportObject %>%
  filter(dummyRow == FALSE) %>% # remove the dummy row
  select(-dummyRow) %>% # remove dummy row designation
  mutate(articleID = articleID) %>% # add the articleID 
  select(articleID, everything()) # make articleID first column

# decide on final outcome
if(any(reportObject$comparisonOutcome != "MATCH") | Insufficient_Information_Errors > 0){
  finalOutcome <- "Failure without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Failure despite author assistance"
  }
}else{
  finalOutcome <- "Success without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Success with author assistance"
  }
}

# collate report extra details
reportExtras <- data.frame(articleID, pilotNames, copilotNames, pilotTTC, copilotTTC, pilotStartDate, copilotStartDate, completionDate, Author_Assistance, finalOutcome, Insufficient_Information_Errors, locus_typo, locus_specification, locus_analysis, locus_data, locus_unidentified)

# save report objects
if(reportType == "pilot"){
  write_csv(reportObject, "pilotReportDetailed.csv")
  write_csv(reportExtras, "pilotReportExtras.csv")
}

if(reportType == "final"){
  write_csv(reportObject, "finalReportDetailed.csv")
  write_csv(reportExtras, "finalReportExtras.csv")
}
```

# Session information

```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
